{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwEcSrF9QFM8",
    "outputId": "7ae7b703-cfe9-4d98-a56f-2cd9aaf7f275"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vibhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vibhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXw2CN1hUOLS",
    "outputId": "f9364a45-c15f-493d-e2ef-cb3a2c0146e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRIRIl__QFM9"
   },
   "outputs": [],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htr6SJMXQFM9"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngaXVxzVQFM_",
    "outputId": "adbca765-95ac-4ea7-f764-1ac1fc913ae1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhu\\AppData\\Local\\Temp\\ipykernel_35724\\3782226354.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data = pd.read_csv(url,index_col = False,sep='\\t+', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "url = \"amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "data = pd.read_csv(url,index_col = False,sep='\\t+', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8znFCyiMQFM_"
   },
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jcw6pxJoQFM_"
   },
   "outputs": [],
   "source": [
    "trimmedData = data[['star_rating','review_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCg7NUMOQFNA"
   },
   "source": [
    " ## We form two classes and select 50000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5zSqf36W49e",
    "outputId": "7b314b76-d444-47d9-b66a-adb372bcefdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmedData['star_rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5aUMtPu-T9KC"
   },
   "outputs": [],
   "source": [
    "trimmedData.loc[trimmedData['star_rating'] <= 3 , 'star_rating'] = 1\n",
    "trimmedData.loc[trimmedData['star_rating'] >= 4,'star_rating'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign class as 1 for rating 1,2,3 and class 2 for rating above 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ksFj3bxnnoze",
    "outputId": "3bd150d5-86f8-42dc-d363-d4a1a97e43f6"
   },
   "outputs": [],
   "source": [
    "data1 = trimmedData[trimmedData['star_rating'] ==1 ].sample(n=50000, random_state=49)\n",
    "data2 = trimmedData[trimmedData['star_rating'] == 2].sample(n=50000, random_state=49)\n",
    "\n",
    "finalData = pd.concat([data1,data2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 50000 records from each class and concat it to make final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTFMCU35QFNB"
   },
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76x6Eob8QFNB"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D2nZey3PqXXK"
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def extend_contractions(text):\n",
    "    extended_words = []\n",
    "    for w in text.split():\n",
    "        extended_words.append(contractions.fix(w))\n",
    "    return ' '.join(extended_words)\n",
    "\n",
    "def text_cleaning(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    #Remove html tags\n",
    "    text = soup.get_text()\n",
    "    #remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n",
    "    #Extend contractions\n",
    "    text = contractions.fix(text)\n",
    "    #Convert lower case\n",
    "    text = text.lower()\n",
    "    #Remove punctuations and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove HTML tags,URL, punctuations and numbers. convert to lower case and extened constractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBxrxzZrksH1",
    "outputId": "3835800a-57df-4af5-e62e-2a889fc93ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN data: 0\n"
     ]
    }
   ],
   "source": [
    "finalData = finalData.drop(finalData[finalData['review_body'].isna()].index)\n",
    "print(\"NaN data: \"+ str(len(finalData[finalData['review_body'].isna()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Nan records and remove them from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTdK6-UDuQAG",
    "outputId": "ae3d339b-7c66-4a9d-c76f-8493f0b9c463"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhu\\AppData\\Local\\Temp\\ipykernel_35724\\3713851753.py:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "finalData['cleaned_review_body'] = finalData['review_body'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCaMFCT0ub2P",
    "outputId": "c3490393-c7be-42ed-c753-d4c3829514cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315.80036800368003 302.4426644266443\n"
     ]
    }
   ],
   "source": [
    "avg_len_before = finalData['review_body'].str.len().mean()\n",
    "avg_len_after = finalData['cleaned_review_body'].str.len().mean()\n",
    "\n",
    "print(str(avg_len_before) + ' ' + str(avg_len_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed URL, html tags, etended contractions and removed punctuations and numbers from reviews. That lead to decrease in average length of reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kcflo-7YQFNC"
   },
   "source": [
    "## remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Uk8hHDOH7VrU"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopWords(text):\n",
    "    word_tokens = text.split(' ')\n",
    "    filtered_words = [x for x in word_tokens if not x.lower() in stop_words]\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IgT8D1uuheEc"
   },
   "outputs": [],
   "source": [
    "finalData['processed_review'] = finalData['cleaned_review_body'].apply(remove_stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords using nltk.corpus and applied it on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyND0_1MQFNC"
   },
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcvPWmLC8QUy",
    "outputId": "1fe466d1-4b86-44e4-f96e-cb5214ec3d80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vibhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xWyL1G2yQFND"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    word_tokens = nltk.tokenize.word_tokenize(text)\n",
    "    for i in range(len(word_tokens)):\n",
    "        word_tokens[i] = lemmatizer.lemmatize(word_tokens[i])\n",
    "    return ' '.join(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V88g85YKOjSw"
   },
   "outputs": [],
   "source": [
    "finalData['processed_review'] = finalData['processed_review'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302.4426644266443 , 191.89438894388945\n"
     ]
    }
   ],
   "source": [
    "avg_len_after_process = finalData['processed_review'].str.len().mean()\n",
    "avg_len_before_process = finalData['cleaned_review_body'].str.len().mean()\n",
    "\n",
    "print(str(avg_len_before_process)+\" , \" + str(avg_len_after_process))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed stop words and performed lemmatization to convert words to normal form. Ex. tried -> try, better -> good\n",
    "\n",
    "significantly decreased the average lenth of review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYnjgpEcQFND"
   },
   "source": [
    "# TF-IDF and BoW Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "D5nzKpCY86-O"
   },
   "outputs": [],
   "source": [
    "X = finalData['review_body']\n",
    "y = finalData['star_rating']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7rjScYFZQFND"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bowVectorizer = CountVectorizer()\n",
    "x_train_bow = bowVectorizer.fit_transform(x_train)\n",
    "x_test_bow = bowVectorizer.transform(x_test)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featurer extraction using bag of words and TF-IDF. Transformed data in the format which can be used in the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qske9_VFQFNE"
   },
   "source": [
    "# Perceptron Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "w4eouibdgcQJ"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.map({1: -1, 2: 1})\n",
    "y_test = y_test.map({1: -1, 2:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wqF5dyNyQFNE"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "bowPerceptron = Perceptron()\n",
    "bowPerceptron.fit(x_train_bow,y_train)\n",
    "y_pred_bow = bowPerceptron.predict(x_test_bow)\n",
    "precision_bow = precision_score(y_test,y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test,y_pred_bow)\n",
    "\n",
    "tfidfPerceptron = Perceptron()\n",
    "tfidfPerceptron.fit(x_train_tfidf,y_train)\n",
    "y_pred_tfidf = tfidfPerceptron.predict(x_test_tfidf)\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test,y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test,y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTyL9yz0jhxm",
    "outputId": "861b7151-7187-44f8-e87f-706e8fabcd1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "0.8157245827010622 0.8638144019282916 0.8390810204380275\n",
      "Tf-IDF\n",
      "0.7753077347051258 0.847644872953701 0.8098642230005277\n"
     ]
    }
   ],
   "source": [
    "print(\"BoW\")\n",
    "print(str(precision_bow) + \" \" + str(recall_bow) + \" \" + str(f1_bow))\n",
    "print(\"Tf-IDF\")\n",
    "print(str(precision_tfidf) + \" \" + str(recall_tfidf) + \" \" + str(f1_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained perceptron using bow and TF-IDF data and got evalution metrics. Tested using the test split. BoW gives better precision with perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnOJfbvTQFNE"
   },
   "source": [
    "# SVM Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90paUfOhQFNF",
    "outputId": "c96401ef-f8aa-490c-abd3-ba142b7c785b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "bowSVC = LinearSVC()\n",
    "bowSVC.fit(x_train_bow,y_train)\n",
    "y_pred_bow = bowSVC.predict(x_test_bow)\n",
    "precision_bow = precision_score(y_test,y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test,y_pred_bow)\n",
    "\n",
    "tfidfSVC = LinearSVC()\n",
    "tfidfSVC.fit(x_train_tfidf,y_train)\n",
    "y_pred_tfidf = tfidfSVC.predict(x_test_tfidf)\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test,y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test,y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va1RzjdXnLWT",
    "outputId": "9345a72a-d852-459f-deaf-3669eace0ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "0.8397222765499707 0.8624083559305011 0.8509141356587226\n",
      "Tf-IDF\n",
      "0.8669432358595568 0.8605001506477855 0.8637096774193549\n"
     ]
    }
   ],
   "source": [
    "print(\"BoW\")\n",
    "print(str(precision_bow) + \" \" + str(recall_bow) + \" \" + str(f1_bow))\n",
    "print(\"Tf-IDF\")\n",
    "print(str(precision_tfidf) + \" \" + str(recall_tfidf) + \" \" + str(f1_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF gives better result on test split when used with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V3OrmQ9QFNF"
   },
   "source": [
    "# Logistic Regression Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvCP-ZgDQFNG",
    "outputId": "011e26f0-7bbe-40fa-ae2b-f625352b7cea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bowReg = LogisticRegression()\n",
    "bowReg.fit(x_train_bow,y_train)\n",
    "y_pred_bow = bowReg.predict(x_test_bow)\n",
    "precision_bow = precision_score(y_test,y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test,y_pred_bow)\n",
    "\n",
    "tfidfReg = LogisticRegression()\n",
    "tfidfReg.fit(x_train_tfidf,y_train)\n",
    "y_pred_tfidf = tfidfReg.predict(x_test_tfidf)\n",
    "precision_tfidf_reg = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf_reg = recall_score(y_test,y_pred_tfidf)\n",
    "f1_tfidf_reg = f1_score(y_test,y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orcmP3d-opGy",
    "outputId": "9cd5dcca-ee67-466f-f0f6-9a030852207e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "0.8502415458937198 0.8838003414683138 0.8666962131284778\n",
      "TF-IDF\n",
      "0.8769938650306749 0.8614040373606509 0.8691290469676243\n"
     ]
    }
   ],
   "source": [
    "print(\"BoW\")\n",
    "print(str(precision_bow) + \" \" + str(recall_bow) + \" \" + str(f1_bow))\n",
    "print(\"TF-IDF\")\n",
    "print(str(precision_tfidf_reg) + \" \" + str(recall_tfidf_reg) + \" \" + str(f1_tfidf_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives similar results as SVM for TFIDF. Works better than SVM for BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVMnDouDQFNG"
   },
   "source": [
    "# Naive Bayes Using Both Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "DNlzOWMtQFNG"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bowNaive = MultinomialNB()\n",
    "bowNaive.fit(x_train_bow,y_train)\n",
    "y_pred_bow = bowNaive.predict(x_test_bow)\n",
    "precision_bow = precision_score(y_test,y_pred_bow)\n",
    "recall_bow = recall_score(y_test, y_pred_bow)\n",
    "f1_bow = f1_score(y_test,y_pred_bow)\n",
    "\n",
    "tfidfNaive = MultinomialNB()\n",
    "tfidfNaive.fit(x_train_tfidf,y_train)\n",
    "y_pred_tfidf = tfidfNaive.predict(x_test_tfidf)\n",
    "precision_tfidf = precision_score(y_test, y_pred_tfidf)\n",
    "recall_tfidf = recall_score(y_test,y_pred_tfidf)\n",
    "f1_tfidf = f1_score(y_test,y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2xW7fglpdss",
    "outputId": "d8988b21-1033-46ca-ead0-91eee51a9545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "0.8032786885245902 0.8661243346389474 0.833518581162712\n",
      "Tf-IDF\n",
      "0.8540012872774083 0.7995380134578688 0.8258727112402096\n"
     ]
    }
   ],
   "source": [
    "print(\"BoW\")\n",
    "print(str(precision_bow) + \" \" + str(recall_bow) + \" \" + str(f1_bow))\n",
    "print(\"Tf-IDF\")\n",
    "print(str(precision_tfidf) + \" \" + str(recall_tfidf) + \" \" + str(f1_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives better results for BoW than TF-IDF"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
